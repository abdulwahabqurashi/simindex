{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and document importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\me1awq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# import docx2txt\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# import textract\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# importing required modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import LineTokenizer\n",
    "nltk.download('punkt')\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "# import docx2txt\n",
    "# import textract\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from docx.api import Document\n",
    "# from tabula import read_pdf\n",
    "# from tabulate import tabulate\n",
    "import numpy as np\n",
    "\n",
    "#pandas max columns and rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv(r\"C:/Users/me1awq/PhD/docsim/datasets/open_source/SICK_train_search_test.csv\", usecols=[1, 2])\n",
    "\n",
    "# Create a new DataFrame with the desired structure\n",
    "main_df = pd.DataFrame(columns=[\"Document name\", \"Document text\"])\n",
    "\n",
    "# Loop through each column in the original DataFrame and append its data to the new DataFrame\n",
    "for col in df.columns:\n",
    "    name = col  # Name of the original column will become the Document name\n",
    "    text = df[col].tolist()  # Data from the column will become the Document text\n",
    "    main_df = main_df.append({\"Document name\": name, \"Document text\": text}, ignore_index=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change the File directory\n",
    "# file_list = glob.glob(os.path.join(os.getcwd(), \"C:/Users/me1awq/PhD/docsim/datasets/rssb\", \"*.csv\"))\n",
    "# corpus = []\n",
    "# files = []\n",
    "\n",
    "# for file_path in file_list:\n",
    "#     with open(file_path, encoding=\"latin-1\") as f_input:\n",
    "#         corpus.append(f_input.read())\n",
    "#         files.append(''.join([n for n in os.path.basename(file_path)]))\n",
    "\n",
    "# main_df = pd.DataFrame({'Document name':files, 'Document text':corpus})\n",
    "# main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the stopwords using nltk library.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "custom_stopwords = [\"ï\",\"»\",\"¿\",\"â€“\",\"â€™\",\"a)\",\"â\",\"¿\",\"–\",\"–\",\"b)\",\"c)\",\"d)\",\"e)\",\":\",\"(\",\")\",\"â€˜\",\"-\",'must','used','using'\n",
    "                   'near']\n",
    "# punctation=[\":\",\"(\",\")\"]\n",
    "stop.extend(custom_stopwords)\n",
    "# stop.extend(punctation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# main_df['Clean_documents_rules'] = main_df['Document text'].str.lower()\n",
    "# main_df['Clean_documents_rules']= main_df['Clean_documents_rules'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def token_sent(text):   \n",
    "#     sent_tokens=LineTokenizer(blanklines='keep').tokenize(text)\n",
    "#     return sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['Document text'] = main_df['Document text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df['Sentence_Tokenize_rules']=main_df['Document text'].apply(tokenize_sentences) \n",
    "# sentence tokenization was not required as sentences are already cleaned and arranged\n",
    "main_df['Sentence_Tokenize_rules']=main_df['Document text']\n",
    "main_df['Word_Tokenize_rules']=main_df['Document text'].apply(tokenize_words) \n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_doc_name = dict(zip(main_df['Document name'], main_df['Sentence_Tokenize_rules']))\n",
    "token_doc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in token_doc_name:\n",
    "    token_doc_name[key] = [s.replace('\\\\', '') for s in token_doc_name[key]]\n",
    "\n",
    "print(token_doc_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#world cloud\n",
    "allwords= ''.join( [rpt for rpt in main_df['Document text']] )\n",
    "wordCloud = WordCloud(background_color=\"white\",width =1600, height=900, random_state =16, \n",
    "                      max_font_size= 150, min_word_length=3,max_words=150,stopwords=stop).generate(allwords)\n",
    "\n",
    "plt.figure(figsize = (20,15), dpi=800)\n",
    "plt.imshow(wordCloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding for all the sentences in the documents\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# all the embedding used for semantic search\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "#model = SentenceTransformer('ddobokki/electra-small-nli-sts')\n",
    "\n",
    "#For Semantic textual similarity for small search queries\n",
    "#model = SentenceTransformer('sentence-transformers/msmarco-distilroberta-base-v2')\n",
    "#model = SentenceTransformer('sentence-transformers/stsb-distilbert-base')\n",
    "# sentence_embeddings = model.encode(sent_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #printing the embedding size and shape\n",
    "# sentence_embeddings1 = model.encode(main_df['Sentence_Tokenize_rules'])\n",
    "# print(\"EA document rule= \",main_df['Sentence_Tokenize_rules'][0])\n",
    "# print(\"Sentence dimension=\",sentence_embeddings1.shape)\n",
    "# print(\"Sentence in vectorized format\",sentence_embeddings1[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the cosine similarity\n",
    "def cosine_sim(embeddings1,embeddings2):\n",
    "    \"\"\"Cosine similarity metric function to calculate the distance between the two vectors.\"\"\"\n",
    "    cossim=( np.dot(embeddings1,embeddings2) )/ (np.linalg.norm(embeddings1)*np.linalg.norm(embeddings2))\n",
    "    if np.isnan(np.sum(cossim)):\n",
    "        return 0\n",
    "    return cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the sentence from all the documents\n",
    "# enter the text you want to search\n",
    "# search_sentence=\"degraded conditions\"\n",
    "# search_sentence=\"2.8.3\"\n",
    "search_sentence=\"A man is playing a harp\"\n",
    "# search_sentence=\"Mitigate risk \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the search sentence\n",
    "search_sentence_embeddings = (model.encode(search_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_sent_tokens=all_tokens\n",
    "# docs_sent_tokens=main_df['Sentence_Tokenize_rules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "docs_sent_tokens=list(chain.from_iterable(main_df['Sentence_Tokenize_rules']))\n",
    "#docs_name=main_df['Document name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "\n",
    "# #set the threshold value to get the similairity result accordingly\n",
    "threshold=0\n",
    "\n",
    "# #embedding all the documents and find the similarity between search text and all the tokenize sentences\n",
    "# for name,docs_sent_token in zip(main_df['Document name'], docs_sent_tokens):\n",
    "for doc_name, docs_sent_tokens in token_doc_name.items():\n",
    "    name=doc_name.title()\n",
    "    for docs_sent_token in docs_sent_tokens:\n",
    "        sentence_embeddings = model.encode(docs_sent_token)\n",
    "        sim_score1 = cosine_sim(search_sentence_embeddings, sentence_embeddings)\n",
    "        if sim_score1 > threshold:\n",
    "            results.append((\n",
    "            docs_sent_token,\n",
    "            sim_score1,\n",
    "            name\n",
    "            ))\n",
    "#             # sorting in descending order based on the similarity score\n",
    "#             #results.sort(key=lambda k: k[1], reverse=True)\n",
    "            \n",
    "            \n",
    "# #results = [set(t) for t in results]\n",
    "\n",
    "# #printing the results saved in the list format\n",
    "# #print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the top 10 matching result in dataframe format\n",
    "df=pd.DataFrame(results, columns=['Matching Sentence','Similarity Score','Document name'])\n",
    "\n",
    "# sorting in descending order based on the similarity score\n",
    "df.sort_values(\"Similarity Score\", ascending = False, inplace = True)\n",
    "\n",
    "#change the value of n to see more results\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
