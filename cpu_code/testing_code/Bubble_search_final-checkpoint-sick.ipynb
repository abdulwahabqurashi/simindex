{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b57ee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\n[WinError 182] The operating system cannot run %1. Error loading \"C:\\Users\\me1awq\\.conda\\envs\\cpu\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\cpu\\lib\\site-packages\\transformers\\utils\\import_utils.py:1093\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\cpu\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\cpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple, Union\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\cpu\\lib\\site-packages\\torch\\__init__.py:128\u001b[0m\n\u001b[0;32m    127\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 182] The operating system cannot run %1. Error loading \"C:\\Users\\me1awq\\.conda\\envs\\cpu\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# from fastapi import FastAPI, HTTPException\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel, BertTokenizer\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\cpu\\lib\\site-packages\\transformers\\utils\\import_utils.py:1084\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1083\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1084\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cpu\\lib\\site-packages\\transformers\\utils\\import_utils.py:1083\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1083\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\cpu\\lib\\site-packages\\transformers\\utils\\import_utils.py:1095\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1098\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\n[WinError 182] The operating system cannot run %1. Error loading \"C:\\Users\\me1awq\\.conda\\envs\\cpu\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# importing required modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import LineTokenizer\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, Any, AnyStr\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "import textract\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from docx.api import Document\n",
    "# from tabula import read_pdf\n",
    "# from tabulate import tabulate\n",
    "import numpy as np\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import csv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#pandas max columns and rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "df = pd.read_csv(r\"C:\\Users\\Omer\\Desktop\\SICK_train2.csv\", usecols=[1, 2])\n",
    "\n",
    "# Create a new DataFrame with the desired structure\n",
    "new_df = pd.DataFrame(columns=[\"Document name\", \"Document text\"])\n",
    "\n",
    "# Loop through each column in the original DataFrame and append its data to the new DataFrame\n",
    "for col in df.columns:\n",
    "    name = col  # Name of the original column will become the Document name\n",
    "    text = df[col].tolist()  # Data from the column will become the Document text\n",
    "    new_df = new_df.append({\"Document name\": name, \"Document text\": text}, ignore_index=True)\n",
    "\n",
    "# Print the new DataFrame\n",
    "new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9f6c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing the stopwords using nltk library.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "custom_stopwords = [\"ï\",\"»\",\"¿\",\"â€“\",\"â€™\",\"a)\",\"â\",\"¿\",\"–\",\"–\",\"b)\",\"c)\",\"d)\",\"e)\",\":\",\"(\",\")\",\"â€˜\",\"-\",'must','used','using'\n",
    "                   'near']\n",
    "# punctation=[\":\",\"(\",\")\"]\n",
    "stop.extend(custom_stopwords)\n",
    "# stop.extend(punctation)\n",
    "\n",
    "\n",
    "new_df['Clean_documents_rules'] = new_df['Document text'].astype(str).str.lower()\n",
    "#new_df['Clean_documents_rules']= new_df['Clean_documents_rules'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b1546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_sent(text):   \n",
    "    sent_tokens=LineTokenizer(blanklines='keep').tokenize(text)\n",
    "    return sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da92f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Sentence_Tokenize_rules']=new_df['Clean_documents_rules'].apply(token_sent) \n",
    "new_df['Word_Tokenize_rules']=new_df['Clean_documents_rules'].apply(word_tokenize) \n",
    "new_df['Sentence_Tokenize_rules'] = new_df['Sentence_Tokenize_rules'].apply(lambda x: [sent.strip() for sent in x])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps document names to sentences\n",
    "token_doc_name = dict(zip(new_df[\"Document name\"], new_df[\"Sentence_Tokenize_rules\"]))\n",
    "token_doc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ba2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding for all the sentences in the documents\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# all the embedding used for semantic search\n",
    "\n",
    "#model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "#model = SentenceTransformer('ddobokki/electra-small-nli-sts')\n",
    "\n",
    "#For Semantic textual similarity for small search queries\n",
    "#model = SentenceTransformer('sentence-transformers/msmarco-distilroberta-base-v2')\n",
    "model = SentenceTransformer('sentence-transformers/stsb-distilbert-base')\n",
    "# sentence_embeddings = model.encode(sent_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f61165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the cosine similarity\n",
    "def cosine_sim(embeddings1,embeddings2):\n",
    "    \"\"\"Cosine similarity metric function to calculate the distance between the two vectors.\"\"\"\n",
    "    cossim=( np.dot(embeddings1,embeddings2) )/ (np.linalg.norm(embeddings1)*np.linalg.norm(embeddings2))\n",
    "    if np.isnan(np.sum(cossim)):\n",
    "        return 0\n",
    "    return cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "docs_sent_tokens=list(chain.from_iterable(new_df['Sentence_Tokenize_rules']))\n",
    "#docs_name=main_df['Document name']\n",
    "docs_sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b0359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# app = FastAPI()\n",
    "\n",
    "output_file = 'search_results_SICK.csv'\n",
    "\n",
    "@app.get(\"/vectorize\")\n",
    "async def vectorize(search_sentence: str):\n",
    "    # Tokenize the sentence\n",
    "    search_sentence_tokens = search_sentence.split(',')\n",
    "    search_sentence_embeddings = [model.encode(sent) for sent in search_sentence_tokens]\n",
    "    results=[]\n",
    "\n",
    "    #set the threshold value to get the similarity result accordingly\n",
    "    threshold=0.5\n",
    "\n",
    "    #embedding all the documents and find the similarity between search text and all the tokenize sentences\n",
    "    for doc_name, docs_sent_tokens in token_doc_name.items():\n",
    "        doc_name = doc_name.title()\n",
    "        for docs_sent_token in docs_sent_tokens:\n",
    "            for search_sentence_embedding, search_sentence_token in zip(search_sentence_embeddings, search_sentence_tokens):\n",
    "                sentence_embeddings = model.encode(docs_sent_token)\n",
    "                sim_score1 = cosine_sim(search_sentence_embedding, sentence_embeddings)\n",
    "                if sim_score1 > threshold:\n",
    "                    results.append((\n",
    "                    search_sentence_token.strip(),\n",
    "                    docs_sent_token.strip(),\n",
    "                    sim_score1,\n",
    "                    search_sentence,\n",
    "                    doc_name\n",
    "                    ))\n",
    "    #printing the top 10 matching result in dataframe format\n",
    "    df=pd.DataFrame(results, columns=['Search Sentence', 'Matching Sentence','Similarity Score', 'Query Name', 'Document name'])\n",
    "\n",
    "    # sorting in descending order based on the similarity score\n",
    "    df.sort_values(\"Similarity Score\", ascending = False, inplace = True)\n",
    "\n",
    "    #change the value of n to see more results\n",
    "    df_top10 = df.head(n=10)\n",
    "\n",
    "    # Create a unique filename using a UUID\n",
    "    filename = 'search_results_SICK.csv'\n",
    "\n",
    "    # Write the top 10 results to a CSV file\n",
    "    df_top10.to_csv(filename, index=False)\n",
    "\n",
    "    # Read the CSV file and return its contents as a dictionary\n",
    "    with open(filename, 'r') as f:\n",
    "        result = csv.DictReader(f)\n",
    "        output = [row for row in result]\n",
    "\n",
    "    return {\"data\": output}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f027f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fce269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2c124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f10b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42f195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
